{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87f0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch.vit_for_small_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516ea373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sSPT(nn.Module):\n",
    "    def __init__(self, *, dim, patch_size, channels = 3):\n",
    "        super().__init__()\n",
    "        patch_dim = patch_size * patch_size * 5 * channels\n",
    "\n",
    "        self.to_patch_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
    "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
    "        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n",
    "        return self.to_patch_tokens(x_with_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adcbb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = sSPT(dim = dim, patch_size = patch_size, channels = channels)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs, timestep_embed):\n",
    "        x = self.to_patch_embedding(imgs)\n",
    "        set_b, b, n, _ = x.shape\n",
    "        x = torch.reshape(x, (set_b*b, n, -1))\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b*set_b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += timestep_embed[:, None, :]\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        x = torch.reshape(x, (set_b, b, -1))\n",
    "        return torch.mean(self.mlp_head(x), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fe7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 16,\n",
    "    num_classes = 1280,\n",
    "    channels=4,\n",
    "    dim = 768,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(4, 4, 32, 32)\n",
    "timestep_embedding = torch.randn(4, 768)\n",
    "preds = v(img, timestep_embedding) # (1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff6159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1280])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb35b1",
   "metadata": {},
   "source": [
    "Pred here is the context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af42b847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42722310"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in v.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ed986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
